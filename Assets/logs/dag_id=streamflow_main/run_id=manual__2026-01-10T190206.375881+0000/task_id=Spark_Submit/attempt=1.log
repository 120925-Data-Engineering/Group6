[2026-01-10T19:02:18.842+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: streamflow_main.Spark_Submit manual__2026-01-10T19:02:06.375881+00:00 [queued]>
[2026-01-10T19:02:18.854+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: streamflow_main.Spark_Submit manual__2026-01-10T19:02:06.375881+00:00 [queued]>
[2026-01-10T19:02:18.854+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 1
[2026-01-10T19:02:18.871+0000] {taskinstance.py:2191} INFO - Executing <Task(BashOperator): Spark_Submit> on 2026-01-10 19:02:06.375881+00:00
[2026-01-10T19:02:18.876+0000] {standard_task_runner.py:60} INFO - Started process 1562 to run task
[2026-01-10T19:02:18.880+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'streamflow_main', 'Spark_Submit', 'manual__2026-01-10T19:02:06.375881+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/dag_streamflow.py', '--cfg-path', '/tmp/tmppyaavq5_']
[2026-01-10T19:02:18.885+0000] {standard_task_runner.py:88} INFO - Job 17: Subtask Spark_Submit
[2026-01-10T19:02:18.945+0000] {task_command.py:423} INFO - Running <TaskInstance: streamflow_main.Spark_Submit manual__2026-01-10T19:02:06.375881+00:00 [running]> on host e997e4968980
[2026-01-10T19:02:19.036+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='student' AIRFLOW_CTX_DAG_ID='streamflow_main' AIRFLOW_CTX_TASK_ID='Spark_Submit' AIRFLOW_CTX_EXECUTION_DATE='2026-01-10T19:02:06.375881+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2026-01-10T19:02:06.375881+00:00'
[2026-01-10T19:02:19.038+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2026-01-10T19:02:19.039+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', '\n            spark-submit                 --master spark://spark-master:7077                 /opt/spark-jobs/etl_job.py\n        ']
[2026-01-10T19:02:19.048+0000] {subprocess.py:86} INFO - Output:
[2026-01-10T19:02:19.055+0000] {subprocess.py:93} INFO - /opt/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2026-01-10T19:02:21.616+0000] {subprocess.py:93} INFO - 26/01/10 19:02:21 INFO SparkContext: Running Spark version 3.5.0
[2026-01-10T19:02:21.618+0000] {subprocess.py:93} INFO - 26/01/10 19:02:21 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2026-01-10T19:02:21.619+0000] {subprocess.py:93} INFO - 26/01/10 19:02:21 INFO SparkContext: Java version 17.0.17
[2026-01-10T19:02:21.684+0000] {subprocess.py:93} INFO - 26/01/10 19:02:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2026-01-10T19:02:21.781+0000] {subprocess.py:93} INFO - 26/01/10 19:02:21 INFO ResourceUtils: ==============================================================
[2026-01-10T19:02:21.782+0000] {subprocess.py:93} INFO - 26/01/10 19:02:21 INFO ResourceUtils: No custom resources configured for spark.driver.
[2026-01-10T19:02:21.783+0000] {subprocess.py:93} INFO - 26/01/10 19:02:21 INFO ResourceUtils: ==============================================================
[2026-01-10T19:02:21.784+0000] {subprocess.py:93} INFO - 26/01/10 19:02:21 INFO SparkContext: Submitted application: ETL_job
[2026-01-10T19:02:21.809+0000] {subprocess.py:93} INFO - 26/01/10 19:02:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2026-01-10T19:02:21.819+0000] {subprocess.py:93} INFO - 26/01/10 19:02:21 INFO ResourceProfile: Limiting resource is cpu
[2026-01-10T19:02:21.821+0000] {subprocess.py:93} INFO - 26/01/10 19:02:21 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2026-01-10T19:02:21.883+0000] {subprocess.py:93} INFO - 26/01/10 19:02:21 INFO SecurityManager: Changing view acls to: root
[2026-01-10T19:02:21.884+0000] {subprocess.py:93} INFO - 26/01/10 19:02:21 INFO SecurityManager: Changing modify acls to: root
[2026-01-10T19:02:21.886+0000] {subprocess.py:93} INFO - 26/01/10 19:02:21 INFO SecurityManager: Changing view acls groups to:
[2026-01-10T19:02:21.887+0000] {subprocess.py:93} INFO - 26/01/10 19:02:21 INFO SecurityManager: Changing modify acls groups to:
[2026-01-10T19:02:21.888+0000] {subprocess.py:93} INFO - 26/01/10 19:02:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
[2026-01-10T19:02:22.161+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO Utils: Successfully started service 'sparkDriver' on port 44627.
[2026-01-10T19:02:22.198+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO SparkEnv: Registering MapOutputTracker
[2026-01-10T19:02:22.246+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO SparkEnv: Registering BlockManagerMaster
[2026-01-10T19:02:22.266+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2026-01-10T19:02:22.267+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2026-01-10T19:02:22.271+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2026-01-10T19:02:22.300+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-51b99d47-41f6-4a63-8b17-3ee832b2ff5e
[2026-01-10T19:02:22.316+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2026-01-10T19:02:22.337+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO SparkEnv: Registering OutputCommitCoordinator
[2026-01-10T19:02:22.498+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2026-01-10T19:02:22.582+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2026-01-10T19:02:22.733+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO Executor: Starting executor ID driver on host e997e4968980
[2026-01-10T19:02:22.735+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2026-01-10T19:02:22.735+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO Executor: Java version 17.0.17
[2026-01-10T19:02:22.749+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2026-01-10T19:02:22.750+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4f3c6780 for default.
[2026-01-10T19:02:22.780+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44521.
[2026-01-10T19:02:22.782+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO NettyBlockTransferService: Server created on e997e4968980:44521
[2026-01-10T19:02:22.785+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2026-01-10T19:02:22.792+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e997e4968980, 44521, None)
[2026-01-10T19:02:22.798+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO BlockManagerMasterEndpoint: Registering block manager e997e4968980:44521 with 434.4 MiB RAM, BlockManagerId(driver, e997e4968980, 44521, None)
[2026-01-10T19:02:22.800+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e997e4968980, 44521, None)
[2026-01-10T19:02:22.802+0000] {subprocess.py:93} INFO - 26/01/10 19:02:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e997e4968980, 44521, None)
[2026-01-10T19:02:23.439+0000] {subprocess.py:93} INFO - 26/01/10 19:02:23 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2026-01-10T19:02:23.452+0000] {subprocess.py:93} INFO - 26/01/10 19:02:23 INFO SharedState: Warehouse path is 'file:/tmp/***tmp_huvzak3/spark-warehouse'.
[2026-01-10T19:02:24.530+0000] {subprocess.py:93} INFO - 26/01/10 19:02:24 INFO InMemoryFileIndex: It took 60 ms to list leaf files for 1 paths.
[2026-01-10T19:02:24.618+0000] {subprocess.py:93} INFO - 26/01/10 19:02:24 INFO InMemoryFileIndex: It took 13 ms to list leaf files for 1 paths.
[2026-01-10T19:02:26.885+0000] {subprocess.py:93} INFO - 26/01/10 19:02:26 INFO FileSourceStrategy: Pushed Filters:
[2026-01-10T19:02:26.896+0000] {subprocess.py:93} INFO - 26/01/10 19:02:26 INFO FileSourceStrategy: Post-Scan Filters:
[2026-01-10T19:02:27.146+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.2 KiB, free 434.2 MiB)
[2026-01-10T19:02:27.198+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 434.2 MiB)
[2026-01-10T19:02:27.202+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on e997e4968980:44521 (size: 34.2 KiB, free: 434.4 MiB)
[2026-01-10T19:02:27.206+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2026-01-10T19:02:27.218+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2026-01-10T19:02:27.372+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2026-01-10T19:02:27.390+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-01-10T19:02:27.392+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2026-01-10T19:02:27.393+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO DAGScheduler: Parents of final stage: List()
[2026-01-10T19:02:27.394+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO DAGScheduler: Missing parents: List()
[2026-01-10T19:02:27.397+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-01-10T19:02:27.480+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KiB, free 434.2 MiB)
[2026-01-10T19:02:27.483+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 434.1 MiB)
[2026-01-10T19:02:27.484+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on e997e4968980:44521 (size: 7.5 KiB, free: 434.4 MiB)
[2026-01-10T19:02:27.485+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
[2026-01-10T19:02:27.500+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-01-10T19:02:27.501+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2026-01-10T19:02:27.545+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (e997e4968980, executor driver, partition 0, PROCESS_LOCAL, 8251 bytes)
[2026-01-10T19:02:27.561+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2026-01-10T19:02:27.654+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO FileScanRDD: Reading File path: file:///opt/spark-data/landing/transaction_events_1768070270.2180772.json, range: 0-105957, partition values: [empty row]
[2026-01-10T19:02:27.910+0000] {subprocess.py:93} INFO - 26/01/10 19:02:27 INFO CodeGenerator: Code generated in 220.33368 ms
[2026-01-10T19:02:28.037+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3040 bytes result sent to driver
[2026-01-10T19:02:28.048+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 514 ms on e997e4968980 (executor driver) (1/1)
[2026-01-10T19:02:28.050+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2026-01-10T19:02:28.056+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0.646 s
[2026-01-10T19:02:28.058+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2026-01-10T19:02:28.059+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2026-01-10T19:02:28.060+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0.687893 s
[2026-01-10T19:02:28.195+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO InMemoryFileIndex: It took 12 ms to list leaf files for 1 paths.
[2026-01-10T19:02:28.227+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO InMemoryFileIndex: It took 11 ms to list leaf files for 1 paths.
[2026-01-10T19:02:28.265+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO FileSourceStrategy: Pushed Filters:
[2026-01-10T19:02:28.267+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO FileSourceStrategy: Post-Scan Filters:
[2026-01-10T19:02:28.270+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.2 KiB, free 434.0 MiB)
[2026-01-10T19:02:28.281+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 433.9 MiB)
[2026-01-10T19:02:28.283+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on e997e4968980:44521 (size: 34.2 KiB, free: 434.3 MiB)
[2026-01-10T19:02:28.286+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2026-01-10T19:02:28.287+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2026-01-10T19:02:28.297+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2026-01-10T19:02:28.299+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-01-10T19:02:28.299+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2026-01-10T19:02:28.300+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO DAGScheduler: Parents of final stage: List()
[2026-01-10T19:02:28.301+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO DAGScheduler: Missing parents: List()
[2026-01-10T19:02:28.302+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-01-10T19:02:28.303+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.0 KiB, free 433.9 MiB)
[2026-01-10T19:02:28.305+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 433.9 MiB)
[2026-01-10T19:02:28.305+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on e997e4968980:44521 (size: 7.5 KiB, free: 434.3 MiB)
[2026-01-10T19:02:28.306+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580
[2026-01-10T19:02:28.307+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-01-10T19:02:28.308+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2026-01-10T19:02:28.309+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (e997e4968980, executor driver, partition 0, PROCESS_LOCAL, 8244 bytes)
[2026-01-10T19:02:28.310+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2026-01-10T19:02:28.320+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO FileScanRDD: Reading File path: file:///opt/spark-data/landing/user_events_1768070270.2175364.json, range: 0-72480, partition values: [empty row]
[2026-01-10T19:02:28.369+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2358 bytes result sent to driver
[2026-01-10T19:02:28.373+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 63 ms on e997e4968980 (executor driver) (1/1)
[2026-01-10T19:02:28.374+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2026-01-10T19:02:28.382+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0.072 s
[2026-01-10T19:02:28.384+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2026-01-10T19:02:28.385+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2026-01-10T19:02:28.386+0000] {subprocess.py:93} INFO - 26/01/10 19:02:28 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0.076646 s
[2026-01-10T19:02:29.114+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO FileSourceStrategy: Pushed Filters: IsNotNull(user_id),IsNotNull(products)
[2026-01-10T19:02:29.116+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(user_id#21),(size(products#12, true) > 0),isnotnull(products#12)
[2026-01-10T19:02:29.120+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO FileSourceStrategy: Pushed Filters: IsNotNull(user_id),IsNotNull(product_id)
[2026-01-10T19:02:29.120+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(user_id#58),isnotnull(product_id#53)
[2026-01-10T19:02:29.167+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO BlockManagerInfo: Removed broadcast_2_piece0 on e997e4968980:44521 in memory (size: 34.2 KiB, free: 434.4 MiB)
[2026-01-10T19:02:29.176+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO BlockManagerInfo: Removed broadcast_1_piece0 on e997e4968980:44521 in memory (size: 7.5 KiB, free: 434.4 MiB)
[2026-01-10T19:02:29.183+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO BlockManagerInfo: Removed broadcast_3_piece0 on e997e4968980:44521 in memory (size: 7.5 KiB, free: 434.4 MiB)
[2026-01-10T19:02:29.188+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO BlockManagerInfo: Removed broadcast_0_piece0 on e997e4968980:44521 in memory (size: 34.2 KiB, free: 434.4 MiB)
[2026-01-10T19:02:29.297+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO CodeGenerator: Code generated in 20.459754 ms
[2026-01-10T19:02:29.300+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.1 KiB, free 434.2 MiB)
[2026-01-10T19:02:29.310+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 434.2 MiB)
[2026-01-10T19:02:29.312+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on e997e4968980:44521 (size: 34.2 KiB, free: 434.4 MiB)
[2026-01-10T19:02:29.314+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2026-01-10T19:02:29.319+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2026-01-10T19:02:29.350+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2026-01-10T19:02:29.352+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO DAGScheduler: Got job 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2026-01-10T19:02:29.353+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO DAGScheduler: Final stage: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2026-01-10T19:02:29.354+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO DAGScheduler: Parents of final stage: List()
[2026-01-10T19:02:29.355+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO DAGScheduler: Missing parents: List()
[2026-01-10T19:02:29.355+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2026-01-10T19:02:29.356+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.6 KiB, free 434.2 MiB)
[2026-01-10T19:02:29.358+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.1 MiB)
[2026-01-10T19:02:29.359+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on e997e4968980:44521 (size: 7.4 KiB, free: 434.4 MiB)
[2026-01-10T19:02:29.360+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580
[2026-01-10T19:02:29.361+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2026-01-10T19:02:29.363+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2026-01-10T19:02:29.364+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (e997e4968980, executor driver, partition 0, PROCESS_LOCAL, 8244 bytes)
[2026-01-10T19:02:29.365+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2026-01-10T19:02:29.402+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO CodeGenerator: Code generated in 12.982632 ms
[2026-01-10T19:02:29.406+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO FileScanRDD: Reading File path: file:///opt/spark-data/landing/user_events_1768070270.2175364.json, range: 0-72480, partition values: [empty row]
[2026-01-10T19:02:29.420+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO CodeGenerator: Code generated in 10.444287 ms
[2026-01-10T19:02:29.459+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO CodeGenerator: Code generated in 6.479443 ms
[2026-01-10T19:02:29.466+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO CodeGenerator: Code generated in 4.604009 ms
[2026-01-10T19:02:29.544+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3221 bytes result sent to driver
[2026-01-10T19:02:29.546+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 183 ms on e997e4968980 (executor driver) (1/1)
[2026-01-10T19:02:29.548+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2026-01-10T19:02:29.548+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO DAGScheduler: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.193 s
[2026-01-10T19:02:29.549+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2026-01-10T19:02:29.550+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2026-01-10T19:02:29.551+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO DAGScheduler: Job 2 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.199038 s
[2026-01-10T19:02:29.574+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO CodeGenerator: Code generated in 8.74484 ms
[2026-01-10T19:02:29.582+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 1026.0 KiB, free 433.1 MiB)
[2026-01-10T19:02:29.595+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 433.1 MiB)
[2026-01-10T19:02:29.596+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on e997e4968980:44521 (size: 2.4 KiB, free: 434.4 MiB)
[2026-01-10T19:02:29.597+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO BlockManagerInfo: Removed broadcast_5_piece0 on e997e4968980:44521 in memory (size: 7.4 KiB, free: 434.4 MiB)
[2026-01-10T19:02:29.598+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2026-01-10T19:02:29.616+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO FileSourceStrategy: Pushed Filters: IsNotNull(user_id),IsNotNull(products)
[2026-01-10T19:02:29.617+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(user_id#21),(size(products#12, true) > 0),isnotnull(products#12)
[2026-01-10T19:02:29.739+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2026-01-10T19:02:29.740+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2026-01-10T19:02:29.740+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2026-01-10T19:02:29.965+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO CodeGenerator: Code generated in 64.708709 ms
[2026-01-10T19:02:29.968+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 199.1 KiB, free 433.0 MiB)
[2026-01-10T19:02:29.983+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 432.9 MiB)
[2026-01-10T19:02:29.985+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on e997e4968980:44521 (size: 34.2 KiB, free: 434.3 MiB)
[2026-01-10T19:02:29.992+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO SparkContext: Created broadcast 7 from csv at NativeMethodAccessorImpl.java:0
[2026-01-10T19:02:29.993+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2026-01-10T19:02:29.999+0000] {subprocess.py:93} INFO - 26/01/10 19:02:29 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2026-01-10T19:02:30.003+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-01-10T19:02:30.003+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)
[2026-01-10T19:02:30.004+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO DAGScheduler: Parents of final stage: List()
[2026-01-10T19:02:30.005+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO DAGScheduler: Missing parents: List()
[2026-01-10T19:02:30.005+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-01-10T19:02:30.040+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 232.5 KiB, free 432.7 MiB)
[2026-01-10T19:02:30.048+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 82.4 KiB, free 432.6 MiB)
[2026-01-10T19:02:30.049+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on e997e4968980:44521 (size: 82.4 KiB, free: 434.3 MiB)
[2026-01-10T19:02:30.050+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580
[2026-01-10T19:02:30.052+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-01-10T19:02:30.053+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2026-01-10T19:02:30.054+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (e997e4968980, executor driver, partition 0, PROCESS_LOCAL, 8251 bytes)
[2026-01-10T19:02:30.055+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2026-01-10T19:02:30.137+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO CodeGenerator: Code generated in 48.601053 ms
[2026-01-10T19:02:30.140+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2026-01-10T19:02:30.142+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2026-01-10T19:02:30.143+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2026-01-10T19:02:30.343+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO FileScanRDD: Reading File path: file:///opt/spark-data/landing/transaction_events_1768070270.2180772.json, range: 0-105957, partition values: [empty row]
[2026-01-10T19:02:30.366+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO CodeGenerator: Code generated in 17.725124 ms
[2026-01-10T19:02:30.376+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO CodeGenerator: Code generated in 5.314521 ms
[2026-01-10T19:02:30.604+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO FileOutputCommitter: Saved output of task 'attempt_202601101902296673160423053365889_0003_m_000000_3' to file:/opt/spark-data/gold/advertising/_temporary/0/task_202601101902296673160423053365889_0003_m_000000
[2026-01-10T19:02:30.605+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO SparkHadoopMapRedUtil: attempt_202601101902296673160423053365889_0003_m_000000_3: Committed. Elapsed time: 48 ms.
[2026-01-10T19:02:30.612+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2850 bytes result sent to driver
[2026-01-10T19:02:30.616+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 563 ms on e997e4968980 (executor driver) (1/1)
[2026-01-10T19:02:30.617+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2026-01-10T19:02:30.618+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 0.615 s
[2026-01-10T19:02:30.618+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2026-01-10T19:02:30.619+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2026-01-10T19:02:30.623+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 0.619143 s
[2026-01-10T19:02:30.624+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO FileFormatWriter: Start to commit write Job f9d1ffc8-c859-499f-b1e6-7bfb8a8ce4ea.
[2026-01-10T19:02:30.861+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO FileFormatWriter: Write Job f9d1ffc8-c859-499f-b1e6-7bfb8a8ce4ea committed. Elapsed time: 240 ms.
[2026-01-10T19:02:30.864+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO FileFormatWriter: Finished processing stats for write job f9d1ffc8-c859-499f-b1e6-7bfb8a8ce4ea.
[2026-01-10T19:02:30.869+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2026-01-10T19:02:30.886+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO SparkUI: Stopped Spark web UI at http://e997e4968980:4040
[2026-01-10T19:02:30.899+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2026-01-10T19:02:30.922+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO MemoryStore: MemoryStore cleared
[2026-01-10T19:02:30.923+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO BlockManager: BlockManager stopped
[2026-01-10T19:02:30.926+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO BlockManagerMaster: BlockManagerMaster stopped
[2026-01-10T19:02:30.928+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2026-01-10T19:02:30.942+0000] {subprocess.py:93} INFO - 26/01/10 19:02:30 INFO SparkContext: Successfully stopped SparkContext
[2026-01-10T19:02:31.164+0000] {subprocess.py:93} INFO - 26/01/10 19:02:31 INFO ShutdownHookManager: Shutdown hook called
[2026-01-10T19:02:31.165+0000] {subprocess.py:93} INFO - 26/01/10 19:02:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-687eb530-c4d5-4b99-88bc-9d07234f2f0b
[2026-01-10T19:02:31.169+0000] {subprocess.py:93} INFO - 26/01/10 19:02:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-eff70104-4710-40c9-9d44-033be7895458/pyspark-a222e507-588f-46ec-a3c4-83afaa2cfb77
[2026-01-10T19:02:31.175+0000] {subprocess.py:93} INFO - 26/01/10 19:02:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-eff70104-4710-40c9-9d44-033be7895458
[2026-01-10T19:02:31.223+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2026-01-10T19:02:31.260+0000] {taskinstance.py:1138} INFO - Marking task as SUCCESS. dag_id=streamflow_main, task_id=Spark_Submit, execution_date=20260110T190206, start_date=20260110T190218, end_date=20260110T190231
[2026-01-10T19:02:31.316+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2026-01-10T19:02:31.332+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
